/**
 * AI Chip Data
 *
 * To update this data:
 * - Add new chips by copying an existing object and modifying values
 * - All numeric fields (tpp, dieArea, pd, etc.) can be null if unknown
 * - controlStatus options: "Controlled", "Controlled (Oct 2023)", "Unknown", "Entity List (Company)"
 * - sources is an array of objects with { name, url } citing where data came from
 */

export const chipData = [
  {
    name: "NVIDIA R100 (estimate)",
    manufacturer: "NVIDIA",
    architecture: "Rubin",
    releaseDate: "2026",
    tpp: 100000,
    pd: 61.43,
    interconnect: "3600 GB/s (NVLink)",
    fp4: "25000 (inference)",
    fp8: 8750,
    fp16: null,
    bf16: null,
    tf32: null,
    int8: null,
    dieArea: "1628 mm² (estimate)",
    processNode: "TSMC N3P",
    hbmCapacity: "288 GB HBM4",
    memoryBandwidth: "22 TB/s",
    tdp: null,
    controlStatus: "Controlled",
    eccn: "3A090.a",
    notes: "Specs and threshold metrics based on intial NVIDIA press releases/blog posts in early 2026. It is unclear if advertised performance figures were dense or sparse (but were assumed sparse in line with NVIDIA tendencies) and die area is unknown, so TPP/PD should be considered rough estimates.",
    sources: [
      { name: "Vera Rubin platform technical blog", url: "https://developer.nvidia.com/blog/inside-the-nvidia-rubin-platform-six-new-chips-one-ai-supercomputer/#rubin_gpu_execution_engine_for_transformer-era_ai" },
      { name: "Semicon (process node)", url: "https://www.semicone.com/article-179.html" },
    ]
  },
  {
    name: "NVIDIA B30A",
    manufacturer: "NVIDIA",
    architecture: "Blackwell",
    releaseDate: "2026",
    tpp: 56000,
    pd: 34.40,
    interconnect: "900 GB/s (NVLink)",
    fp4: 7000,
    fp8: 2250,
    fp16: 1125,
    bf16: 1125,
    tf32: 550,
    int8: null,
    dieArea: "814 mm² (estimate)",
    processNode: "TSMC N4P",
    hbmCapacity: "135 GB HBM3e",
    memoryBandwidth: "7.7 TB/s",
    tdp: null,
    controlStatus: "Controlled",
    eccn: "3A090.a",
    notes: "Specs and threshold metrics based on rumors and analysis in the cited IFP report that the B30A would be 1/2 of a B300 (one logic die rather than two, 4 HBM stacks rather than 8), which would likely produce 1/2 of the B300’s performance. However, all specs and the TPP/PD should be considered estimates. This chip is included for the sake of completeness and to note that the Trump Administration is reportedly considering permitting sales of the B30A following lobbying efforts from NVIDIA.",
    sources: [
      { name: "IFP, “Should the US Sell Blackwell Chips to China”", url: "https://ifp.org/the-b30a-decision/" },
      ]
  },
  {
    name: "NVIDIA B300",
    manufacturer: "NVIDIA",
    architecture: "Blackwell",
    releaseDate: "2025",
    tpp: 56000,
    pd: 34.40,
    interconnect: "1800 GB/s (NVLink)",
    fp4: 14000,
    fp8: 4500,
    fp16: 2250,
    bf16: 2250,
    tf32: 1100,
    int8: null,
    dieArea: "1628 mm² (estimate)",
    processNode: "TSMC N4P",
    hbmCapacity: "270 GB HBM3e",
    memoryBandwidth: "7.7 TB/s",
    tdp: "1000W",
    controlStatus: "Controlled",
    eccn: "3A090.a",
    notes: "While the B300 has identical arithmetic performance for most bit lengths as the B200, Blackwell Ultra improved dense performance for FP4, so the B300 TPP is ~1.6x the B200. For INT8, the official Blackwell Ultra datasheet performance figure appears erroneous and has been exluded from the data above.",
    sources: [
      { name: "Blackwell Ultra technical brief (performance, specs)", url: "https://resources.nvidia.com/en-us-blackwell-architecture/blackwell-ultra-datasheet?ncid=no-ncid" },
      { name: "Blackwell technical brief (process node)", url: "https://www.tech-odyssey.cn/pdf/nv-gpu/NVIDIA-Blackwell-Architecture-Technical-Overview.pdf#page=6" },
      { name: "Tom’s Hardware (die area estimate)", url: "https://www.tomshardware.com/pc-components/gpus/nvidias-next-gen-ai-gpu-revealed-blackwell-b200-gpu-delivers-up-to-20-petaflops-of-compute-and-massive-improvements-over-hopper-h100#:~:text=The%20reason%20for%20this%20dual,and%20Ada%20Lovelace%20architecture%20GPUs." },
    ]
  },
  {
    name: "NVIDIA B200",
    manufacturer: "NVIDIA",
    architecture: "Blackwell",
    releaseDate: "2024",
    tpp: 36000,
    pd: 22.11,
    interconnect: "1800 GB/s (NVLink)",
    fp4: 9000,
    fp8: 4500,
    fp16: 2250,
    bf16: 2250,
    tf32: 1100,
    int8: 2500,
    dieArea: "1628 mm² (estimate)",
    processNode: "TSMC N4P",
    hbmCapacity: "192 GB HBM3e",
    memoryBandwidth: "8 TB/s",
    tdp: "1000W",
    controlStatus: "Controlled",
    eccn: "3A090.a",
    notes: "—",
    sources: [
      { name: "Blackwell technical brief (performance, specs, process node)", url: "https://www.tech-odyssey.cn/pdf/nv-gpu/NVIDIA-Blackwell-Architecture-Technical-Overview.pdf#page=19" },
      { name: "Tom’s Hardware (die area estimate)", url: "https://www.tomshardware.com/pc-components/gpus/nvidias-next-gen-ai-gpu-revealed-blackwell-b200-gpu-delivers-up-to-20-petaflops-of-compute-and-massive-improvements-over-hopper-h100#:~:text=The%20reason%20for%20this%20dual,and%20Ada%20Lovelace%20architecture%20GPUs." },
    ]
  },
  {
    name: "NVIDIA B100",
    manufacturer: "NVIDIA",
    architecture: "Blackwell",
    releaseDate: "2024",
    tpp: 28000,
    pd: 17.20,
    interconnect: "1800 GB/s (NVLink)",
    fp4: 7000,
    fp8: 3500,
    fp16: 1750,
    bf16: 1750,
    tf32: 900,
    int8: 3500,
    dieArea: "1628 mm² (estimate)",
    processNode: "TSMC N4P",
    hbmCapacity: "192 GB HBM3e",
    memoryBandwidth: "8 TB/s",
    tdp: "700W",
    controlStatus: "Controlled",
    eccn: "3A090.a",
    notes: "—",
    sources: [
      { name: "Blackwell technical brief (performance, specs, process node)", url: "https://www.tech-odyssey.cn/pdf/nv-gpu/NVIDIA-Blackwell-Architecture-Technical-Overview.pdf#page=19" },
      { name: "Tom’s Hardware (die area estimate)", url: "https://www.tomshardware.com/pc-components/gpus/nvidias-next-gen-ai-gpu-revealed-blackwell-b200-gpu-delivers-up-to-20-petaflops-of-compute-and-massive-improvements-over-hopper-h100#:~:text=The%20reason%20for%20this%20dual,and%20Ada%20Lovelace%20architecture%20GPUs." },
    ]
  },
  {
    name: "NVIDIA H200 NVL",
    manufacturer: "NVIDIA",
    architecture: "Hopper",
    releaseDate: "2024",
    tpp: 13364,
    pd: 16.42,
    interconnect: "900 GB/s (NVLink)",
    fp4: null,
    fp8: 1670.5,
    fp16: 835.5,
    bf16: 835.5,
    tf32: 417.5,
    int8: 1670.5,
    dieArea: "814 mm²",
    processNode: "TSMC N4",
    hbmCapacity: "141 GB HBM3e",
    memoryBandwidth: "4.8 TB/s",
    tdp: "600W",
    controlStatus: "Exportable\n(special exception)",
    eccn: null,
    notes: "In December 2025, the Trump Administration announced that Nvidia will be allowed to sell H200 chips to China in exchange for a 25% surcharge.",
    sources: [
      { name: "H200 datasheet (performance, specs)", url: "https://resources.nvidia.com/en-us-data-center-overview/hpc-datasheet-sc23-h200" },
      { name: "NVIDIA Technical Blog (die area and process node)", url: "https://developer.nvidia.com/blog/nvidia-hopper-architecture-in-depth/" },    
      { name: "Semafor (Trump deal)", url: "https://www.semafor.com/article/12/09/2025/trump-says-nvidia-can-sell-h200-ai-chips-to-china" }
    ]
  },
  {
    name: "NVIDIA H200 SXM",
    manufacturer: "NVIDIA",
    architecture: "Hopper",
    releaseDate: "2024",
    tpp: 15832,
    pd: 19.45,
    interconnect: "900 GB/s (NVLink)",
    fp4: null,
    fp8: 1979,
    fp16: 989.5,
    bf16: 989.5,
    tf32: 494.5,
    int8: 1979,
    dieArea: "814 mm²",
    processNode: "TSMC N4",
    hbmCapacity: "141 GB HBM3e",
    memoryBandwidth: "4.8 TB/s",
    tdp: "700W",
    controlStatus: "Exportable\n(special exception)",
    eccn: null,
    notes: "In December 2025, the Trump Administration announced that Nvidia will be allowed to sell H200 chips to China in exchange for a 25% surcharge.",
    sources: [
      { name: "H200 datasheet (performance, specs)", url: "https://resources.nvidia.com/en-us-data-center-overview/hpc-datasheet-sc23-h200" },
      { name: "NVIDIA Technical Blog (die area and process node)", url: "https://developer.nvidia.com/blog/nvidia-hopper-architecture-in-depth/" },    
      { name: "Semafor (Trump deal)", url: "https://www.semafor.com/article/12/09/2025/trump-says-nvidia-can-sell-h200-ai-chips-to-china" }
    ]
  },
  {
    name: "NVIDIA H20 SXM",
    manufacturer: "NVIDIA",
    architecture: "Hopper",
    releaseDate: "2024",
    tpp: 2368,
    pd: 2.91,
    interconnect: "900 GB/s (NVLink)",
    fp4: null,
    fp8: 296,
    fp16: 148,
    bf16: 148,
    tf32: 74,
    int8: 296,
    dieArea: "814 mm²",
    processNode: "TSMC N4",
    hbmCapacity: "96 GB HBM3",
    memoryBandwidth: "4 TB/s",
    tdp: "400W",
    controlStatus: "Exportable\n(special exception)",
    eccn: "3A090.b.2",
    notes: "China-specific Hopper chip with downgraded arithmetic performance to fall under TPP and PD thresholds in light of 2023 controls. The H20’s inference-optimized features (enhanced interconnect and memory capacity/bandwidth) made it an unexpectedly powerful chip with the rise of reasoning models that leverage test-time compute scaling. While the TPP and PD metrics would (at least) make the H20 NAC/ACA eligible, the US government concluded that the export of H20s violates the supercomputer end-use restrictions under 15 C.F.R § 744.23(a)(1). However, the Trump Administration instead opted to allow exports in exchange for 15% of Nvidia’s revenue from these sales.",
    sources: [
      { name: "Tom’s Hardware (performance, specs)", url: "https://www.tomshardware.com/news/nvidias-latest-regulation-compliant-gpu-for-china-has-been-delayed-to-early-next-year" },
      { name: "NVIDIA Technical Blog (die area and process node)", url: "https://developer.nvidia.com/blog/nvidia-hopper-architecture-in-depth/" },
      { name: "IFP (supercomputer end-use analysis)", url: "https://ifp.org/the-h20-problem/" },        
      { name: "NPR (Trump deal)", url: "https://www.npr.org/2025/08/11/nx-s1-5498689/trump-nvidia-h20-chip-sales-china" },    
    ]
  },
  {
    name: "NVIDIA H800 PCIe",
    manufacturer: "NVIDIA",
    architecture: "Hopper",
    releaseDate: "2023",
    tpp: 12104,
    pd: 14.87,
    interconnect: "400 GB/s (NVLink)",
    fp4: null,
    fp8: 1513,
    fp16: 756.5,
    bf16: 756.5,
    tf32: 378,
    int8: 1513,
    dieArea: "814 mm²",
    processNode: "—",
    hbmCapacity: "80 GB HBM3",
    memoryBandwidth: "2 TB/s",
    tdp: "350W",
    controlStatus: "Controlled",
    eccn: "3A090.a",
    notes: "China-specific chip with same arithmetic performance as H100 but with reduced interconnect designed to circumvent 2022 export controls. Loophole closed in 2023 controls with removal of interconnect threshold and introduction of PD.",
    sources: [
      { name: "H800 datasheet (performance, specs)", url: "https://www.chaoqing-i.com/upload/20231128/NVIDIA%20H800%20GPU%20Datasheet.pdf" },
      { name: "NVIDIA Technical Blog (die area and process node)", url: "https://developer.nvidia.com/blog/nvidia-hopper-architecture-in-depth/" },
    ]
  },
  {
    name: "NVIDIA H800 SXM",
    manufacturer: "NVIDIA",
    architecture: "Hopper",
    releaseDate: "2023",
    tpp: 15832,
    pd: 19.45,
    interconnect: "400 GB/s (NVLink)",
    fp4: null,
    fp8: 1979,
    fp16: 989.5,
    bf16: 989.5,
    tf32: 494.5,
    int8: 1979,
    dieArea: "814 mm²",
    processNode: "TSMC N4",
    hbmCapacity: "80 GB HBM3",
    memoryBandwidth: "3.35 TB/s",
    tdp: "700W",
    controlStatus: "Controlled",
    eccn: "3A090.a",
    notes: "China-specific chip with same arithmetic performance as H100 but with reduced interconnect designed to circumvent 2022 export controls. Loophole closed in 2023 controls with removal of interconnect threshold and introduction of PD.",
    sources: [
    { name: "H800 datasheet (performance, specs)", url: "https://www.chaoqing-i.com/upload/20231128/NVIDIA%20H800%20GPU%20Datasheet.pdf" },
    { name: "NVIDIA Technical Blog (die area and process node)", url: "https://developer.nvidia.com/blog/nvidia-hopper-architecture-in-depth/" },
    ]
  },
  {
    name: "NVIDIA H100 NVL",
    manufacturer: "NVIDIA",
    architecture: "Hopper",
    releaseDate: "2022",
    tpp: 13364,
    pd: 16.42,
    interconnect: "600 GB/s (NVLink)",
    fp4: null,
    fp8: 1670.5,
    fp16: 835.5,
    bf16: 835.5,
    tf32: 417.5,
    int8: 1670.5,
    dieArea: "814 mm²",
    processNode: "TSMC N4",
    hbmCapacity: "80 GB HBM3",
    memoryBandwidth: "3.9 TB/s",
    tdp: "400W",
    controlStatus: "Controlled",
    eccn: "3A090.a",
    notes: "—",
    sources: [
      { name: "H100 datasheet (performance, specs)", url: "https://resources.nvidia.com/en-us-gpu/h100-datasheet-24306" },
      { name: "NVIDIA Technical Blog (die area and process node)", url: "https://developer.nvidia.com/blog/nvidia-hopper-architecture-in-depth/" },
        ]
  },
  {
    name: "NVIDIA H100 SXM",
    manufacturer: "NVIDIA",
    architecture: "Hopper",
    releaseDate: "2022",
    tpp: 15832,
    pd: 19.45,
    interconnect: "900 GB/s (NVLink)",
    fp4: null,
    fp8: 1979,
    fp16: 989.5,
    bf16: 989.5,
    tf32: 494.5,
    int8: 1979,
    dieArea: "814 mm²",
    processNode: "TSMC N4",
    hbmCapacity: "80 GB HBM3",
    memoryBandwidth: "3.35 TB/s",
    tdp: "700W",
    controlStatus: "Controlled",
    eccn: "3A090.a",
    notes: "—",
    sources: [
    { name: "H100 datasheet (performance, specs)", url: "https://resources.nvidia.com/en-us-gpu/h100-datasheet-24306" },
    { name: "NVIDIA Technical Blog (die area and process node)", url: "https://developer.nvidia.com/blog/nvidia-hopper-architecture-in-depth/" },
    ]
  },
  {
    name: "NVIDIA A800 PCIe",
    manufacturer: "NVIDIA",
    architecture: "Ampere",
    releaseDate: "2022",
    tpp: 4992,
    pd: 6.04,
    interconnect: "400 GB/s (NVLink)",
    fp4: null,
    fp8: null,
    fp16: 312,
    bf16: 312,
    tf32: 156,
    int8: 624,
    dieArea: "826 mm²",
    processNode: "TSMC N7",
    hbmCapacity: "80 GB HBM2e",
    memoryBandwidth: "1.9 TB/s",
    tdp: "250W",
    controlStatus: "Controlled",
    eccn: "3A090.a",
    notes: "China-specific chip with same arithmetic performance as A100 but with reduced interconnect designed to circumvent 2022 export controls. Loophole closed in 2023 controls with removal of interconnect threshold and introduction of PD.",
    sources: [
      { name: "A800 datasheet (performance, specs)", url: "https://img.deepbaytech.com/NVIDIA%20A800%20TENSOR%20CORE%20GPU.pdf" },
      { name: "Ampere white paper (die area and process node)", url: "https://images.nvidia.com/aem-dam/en-zz/Solutions/data-center/nvidia-ampere-architecture-whitepaper.pdf#page=14" },
    ]
  },
  {
    name: "NVIDIA A800 SXM",
    manufacturer: "NVIDIA",
    architecture: "Ampere",
    releaseDate: "2022",
    tpp: 4992,
    pd: 6.04,
    interconnect: "400 GB/s (NVLink)",
    fp4: null,
    fp8: null,
    fp16: 312,
    bf16: 312,
    tf32: 156,
    int8: 624,
    dieArea: "826 mm²",
    processNode: "TSMC N7",
    hbmCapacity: "80 GB HBM2e",
    memoryBandwidth: "2 TB/s",
    tdp: "400W",
    controlStatus: "Controlled",
    eccn: "3A090.a",
    notes: "China-specific chip with same arithmetic performance as H100 but with reduced interconnect designed to circumvent 2022 export controls. Loophole closed in 2023 controls with removal of interconnect threshold and introduction of PD.",
    sources: [
      { name: "A800 datasheet (performance, specs)", url: "https://img.deepbaytech.com/NVIDIA%20A800%20TENSOR%20CORE%20GPU.pdf" },
      { name: "Ampere white paper (die area and process node)", url: "https://images.nvidia.com/aem-dam/en-zz/Solutions/data-center/nvidia-ampere-architecture-whitepaper.pdf#page=14" },
    ]
  },
  {
    name: "NVIDIA A100 PCIe",
    manufacturer: "NVIDIA",
    architecture: "Ampere",
    releaseDate: "2020",
    tpp: 4992,
    pd: 6.04,
    interconnect: "600 GB/s (NVLink)",
    fp4: null,
    fp8: null,
    fp16: 312,
    bf16: 312,
    tf32: 156,
    int8: 624,
    dieArea: "826 mm²",
    processNode: "TSMC N7",
    hbmCapacity: "40 GB HBM2",
    memoryBandwidth: "1.6 TB/s",
    tdp: "250W",
    controlStatus: "Controlled",
    eccn: "3A090.a",
    notes: "Primary target of 2022 controls, with intial thresholds essentially set right at A100 levels (TPP limit of 4800 right below A100 TPP, interconnect limit of 600 GB/s right at A100 mark).",
    sources: [
      { name: "A100 datasheet (performance, specs)", url: "https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/a100/pdf/nvidia-a100-datasheet.pdf" },
      { name: "Ampere white paper (die area and process node)", url: "https://images.nvidia.com/aem-dam/en-zz/Solutions/data-center/nvidia-ampere-architecture-whitepaper.pdf#page=14" },
    ]
  },
  {
    name: "NVIDIA A100 SXM",
    manufacturer: "NVIDIA",
    architecture: "Ampere",
    releaseDate: "2020",
    tpp: 4992,
    pd: 6.04,
    interconnect: "600 GB/s (NVLink)",
    fp4: null,
    fp8: null,
    fp16: 312,
    bf16: 312,
    tf32: 156,
    int8: 624,
    dieArea: "826 mm²",
    processNode: "TSMC N7",
    hbmCapacity: "40 GB HBM2",
    memoryBandwidth: "1.6 TB/s",
    tdp: "400W",
    controlStatus: "Controlled",
    eccn: "3A090.a",
    notes: "Primary target of 2022 controls, with intial thresholds essentially set right at A100 levels (TPP limit of 4800 right below A100 TPP, interconnect limit of 600 GB/s right at A100 mark).",
    sources: [
      { name: "A100 datasheet (performance, specs)", url: "https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/a100/pdf/nvidia-a100-datasheet.pdf" },
      { name: "Ampere white paper (die area and process node)", url: "https://images.nvidia.com/aem-dam/en-zz/Solutions/data-center/nvidia-ampere-architecture-whitepaper.pdf#page=14" },
    ]
  },
  {
    name: "AMD MI355X",
    manufacturer: "AMD",
    architecture: "CDNA 4",
    releaseDate: "2025",
    tpp: 40265,
    pd: 39.59,
    interconnect: "1075.2 GB/s (Infinity Fabric)",
    fp4: 10066.3,
    fp8: 5033.2,
    fp16: 2516.6,
    bf16: 2516.6,
    tf32: null,
    int8: 5033.2,
    dieArea: "1017 mm² (estimate)",
    processNode: "TSMC N3P, N6",
    hbmCapacity: "288 GB HBM3e",
    memoryBandwidth: "8 TB/s",
    tdp: "1400W",
    controlStatus: "Controlled",
    eccn: "3A090.a",
    notes: "—",
    sources: [
    { name: "MI355X datasheet (performance, specs)", url: "https://www.amd.com/content/dam/amd/en/documents/instinct-tech-docs/product-briefs/amd-instinct-mi355x-gpu-brochure.pdf" },    
    { name: "TechPowerUp (die area)", url: "https://www.techpowerup.com/gpu-specs/amd-cdna-next.g1093" },    
    { name: "CDNA 4 white paper (process node)", url: "https://www.amd.com/content/dam/amd/en/documents/instinct-tech-docs/white-papers/amd-cdna-4-architecture-whitepaper.pdf#page=4" },      
  ]
  },
  {
    name: "AMD MI350X",
    manufacturer: "AMD",
    architecture: "CDNA 4",
    releaseDate: "2025",
    tpp: 36910,
    pd: 36.29,
    interconnect: "896 GB/s (Infinity Fabric)",
    fp4: 9227.5,
    fp8: 4614,
    fp16: 2309.6,
    bf16: 2309.6,
    tf32: null,
    int8: 4614,
    dieArea: "1017 mm² (estimate)",
    processNode: "TSMC N3P, N6",
    hbmCapacity: "288 GB HBM3e",
    memoryBandwidth: "8 TB/s",
    tdp: "1000W",
    controlStatus: "Controlled",
    eccn: "3A090.a",
    notes: "—",
    sources: [
    { name: "MI350X datasheet (performance, specs)", url: "https://www.amd.com/content/dam/amd/en/documents/instinct-tech-docs/product-briefs/amd-instinct-mi350x-gpu-brochure.pdf" }, 
    { name: "TechPowerUp (die area)", url: "https://www.techpowerup.com/gpu-specs/amd-cdna-next.g1093" },    
    { name: "CDNA 4 white paper (process node)", url: "https://www.amd.com/content/dam/amd/en/documents/instinct-tech-docs/white-papers/amd-cdna-4-architecture-whitepaper.pdf#page=4" },    
  ]
  },
   {
    name: "AMD MI308 (estimate)",
    manufacturer: "AMD",
    architecture: "CDNA 3",
    releaseDate: "2024",
    tpp: 2368,
    pd: 2.9,
    interconnect: "—",
    fp4: null,
    fp8: null,
    fp16: null,
    bf16: null,
    tf32: null,
    int8: null,
    dieArea: "1017 mm² (estimate)",
    processNode: "TSMC N5 or N5P (estimate)",
    hbmCapacity: null,
    memoryBandwidth: null,
    tdp: null,
    controlStatus: "Exportable (special exception)",
    eccn: "3A090.b.2",
    notes: "NOTE: The MI308 specs are not public, so the TPP and PD listed above are based on assumptions that its performance is “likely to be close[]” to that of the NVIDIA H20. The threshold values should therefore not be taken as accurate. The MI308 is included here for the sake of completeness and to note that this chip—a China-specific CDNA 3 GPU serving essentially the same purpose of providing an export controls-compliant offering for the Chinese market as the H20 served for NVIDIA’s Hopper lineup—was permitted to be exported to China under the same 15% profit-sharing arrangement as the H20.",
    sources: [
    { name: "WCCF TECH (Trump deal, estimated specs)", url: "https://wccftech.com/amd-gears-up-for-a-major-win-as-its-instinct-mi308-nears-export-approval-for-china/" },
    { name: "TechPowerUp (Trump deal)", url: "https://www.techpowerup.com/338945/amd-cleared-to-resume-mi308-ai-gpu-sales-in-china" },    
  ]
  }, 
  {
    name: "AMD MI325X",
    manufacturer: "AMD",
    architecture: "CDNA 3",
    releaseDate: "2024",
    tpp: 20919,
    pd: 20.57,
    interconnect: "896 GB/s (Infinity Fabric)",
    fp4: null,
    fp8: 2614.9,
    fp16: 1307.4,
    bf16: 1307.4,
    tf32: 653.7,
    int8: 2614.9,
    dieArea: "1017 mm² (estimate)",
    processNode: "TSMC N5 or N5P (estimate)",
    hbmCapacity: "256 GB HBM3e",
    memoryBandwidth: "6 TB/s",
    tdp: "1000W",
    controlStatus: "Controlled",
    eccn: "3A090.a",
    notes: "—",
    sources: [
    { name: "MI325X datasheet (performance, specs)", url: "https://www.amd.com/content/dam/amd/en/documents/instinct-tech-docs/product-briefs/instinct-mi325x-datasheet.pdf" },
    { name: "TechPowerUp (die area)", url: "https://www.techpowerup.com/gpu-specs/radeon-instinct-mi300x.c4179" },    
    { name: "Tom’s Hardware (process node)", url: "https://www.tomshardware.com/news/amd-cdna-3-mi300-apu" },
  ]
  },
  {
    name: "AMD MI300X",
    manufacturer: "AMD",
    architecture: "CDNA 3",
    releaseDate: "2023",
    tpp: 20919,
    pd: 20.57,
    interconnect: "896 GB/s (Infinity Fabric)",
    fp4: null,
    fp8: 2614.9,
    fp16: 312,
    bf16: 312,
    tf32: 653.7,
    int8: 2614.9,
    dieArea: "1017 mm² (estimate)",
    processNode: "TSMC N5 or N5P (estimate)",
    hbmCapacity: "192 GB HBM3",
    memoryBandwidth: "5.3 TB/s",
    tdp: "750W",
    controlStatus: "Controlled",
    eccn: "3A090.a",
    notes: "—",
    sources: [
    { name: "MI300X datasheet (performance, specs)", url: "https://www.amd.com/content/dam/amd/en/documents/instinct-tech-docs/data-sheets/amd-instinct-mi300x-data-sheet.pdf" },
    { name: "TechPowerUp (die area)", url: "https://www.techpowerup.com/gpu-specs/radeon-instinct-mi300x.c4179" },
    { name: "Tom’s Hardware (process node)", url: "https://www.tomshardware.com/news/amd-cdna-3-mi300-apu" },
  ]
  },
  {
    name: "AMD MI300A",
    manufacturer: "AMD",
    architecture: "CDNA 3",
    releaseDate: "2023",
    tpp: 15690,
    pd: 15.43,
    interconnect: "512 GB/s (Infinity Fabric)",
    fp4: null,
    fp8: 1961.2,
    fp16: 980.6,
    bf16: 980.6,
    tf32: 490.3,
    int8: 1961.2,
    dieArea: "1017 mm² (estimate)",
    processNode: "TSMC N5 or N5P (estimate)",
    hbmCapacity: "128 GB HBM3",
    memoryBandwidth: "5.3 TB/s",
    tdp: "760W",
    controlStatus: "Controlled",
    eccn: "3A090.a",
    notes: "—",
    sources: [
    { name: "MI300A datasheet (performance, specs)", url: "https://www.amd.com/content/dam/amd/en/documents/instinct-tech-docs/data-sheets/amd-instinct-mi300a-data-sheet.pdf" },
    { name: "TechPowerUp (die area)", url: "https://www.techpowerup.com/gpu-specs/radeon-instinct-mi300x.c4179" },
    { name: "Tom’s Hardware (process node)", url: "https://www.tomshardware.com/news/amd-cdna-3-mi300-apu" },
    ]
  },
  {
    name: "AMD MI250X",
    manufacturer: "AMD",
    architecture: "CDNA 2",
    releaseDate: "2021",
    tpp: 6128,
    pd: 3.88,
    interconnect: "100 GB/s (Infinity Fabric)",
    fp4: null,
    fp8: null,
    fp16: 383,
    bf16: 383,
    tf32: null,
    int8: 383,
    dieArea: "1580 mm² (estimate)",
    processNode: "TSMC N6",
    hbmCapacity: "128 GB HBM2e",
    memoryBandwidth: "3.2 TB/s",
    tdp: "560W",
    controlStatus: "Controlled",
    eccn: "3A090.a",
    notes: "—",
    sources: [
    { name: "MI250X product page (performance, specs)", url: "https://www.amd.com/en/products/accelerators/instinct/mi200/mi250x.html" },
    { name: "Tom’s Hardware (die area)", url: "https://www.tomshardware.com/news/amd-instinct-mi250x-pictured" },
    { name: "TechPowerUp (process node)", url: "https://www.techpowerup.com/298100/amd-releases-its-cdna2-mi250x-aldebaran-hpc-gpu-block-diagram" },
    ]
  },
  {
    name: "AMD MI250",
    manufacturer: "AMD",
    architecture: "CDNA 2",
    releaseDate: "2021",
    tpp: 5794,
    pd: 3.67,
    interconnect: "100 GB/s (Infinity Fabric)",
    fp4: null,
    fp8: null,
    fp16: 362.1,
    bf16: 362.1,
    tf32: null,
    int8: 362.1,
    dieArea: "1580 mm² (estimate)",
    processNode: "TSMC N6",
    hbmCapacity: "128 GB HBM2e",
    memoryBandwidth: "3.2 TB/s",
    tdp: "560W",
    controlStatus: "Controlled",
    eccn: "3A090.a",
    notes: "—",
    sources: [
    { name: "MI250 product page (performance, specs)", url: "https://www.amd.com/en/products/accelerators/instinct/mi200/mi250.html" },
    { name: "Tom’s Hardware (die area)", url: "https://www.tomshardware.com/news/amd-instinct-mi250x-pictured" },
    { name: "TechPowerUp (process node)", url: "https://www.techpowerup.com/298100/amd-releases-its-cdna2-mi250x-aldebaran-hpc-gpu-block-diagram" },
    ]
  },
  {
    name: "AMD MI210",
    manufacturer: "AMD",
    architecture: "CDNA 2",
    releaseDate: "2022",
    tpp: 2896,
    pd: 3.67,
    interconnect: "100 GB/s (Infinity Fabric)",
    fp4: null,
    fp8: null,
    fp16: 181,
    bf16: 181,
    tf32: null,
    int8: 181,
    dieArea: "790 mm² (estimate)",
    processNode: "TSMC N6",
    hbmCapacity: "64 GB HBM2e",
    memoryBandwidth: "1.6 TB/s",
    tdp: null,
    controlStatus: "NAC/ACA eligible",
    eccn: "3A090.b",
    notes: "—",
    sources: [
    { name: "MI250 product page (performance, specs)", url: "https://www.amd.com/en/products/accelerators/instinct/mi200/mi250.html" },
    { name: "Tom’s Hardware (die area)", url: "https://www.tomshardware.com/news/amd-instinct-mi250x-pictured" },
    { name: "TechPowerUp (process node)", url: "https://www.techpowerup.com/298100/amd-releases-its-cdna2-mi250x-aldebaran-hpc-gpu-block-diagram" },
    ]
  },
];
