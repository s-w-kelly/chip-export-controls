/**
 * AI Chip Data
 *
 * To update this data:
 * - Add new chips by copying an existing object and modifying values
 * - All numeric fields (tpp, dieArea, pd, etc.) can be null if unknown
 * - controlStatus options: "Controlled", "Controlled (Oct 2023)", "Unknown", "Entity List (Company)"
 * - sources is an array of objects with { name, url } citing where data came from
 */

export const chipData = [
  {
    name: "NVIDIA B200",
    manufacturer: "NVIDIA",
    architecture: "Blackwell",
    releaseDate: "2024",
    tpp: 36000,
    interconnect: 1614,
    pd: 22.3,
    fp4: 1,
    fp8: 4500,
    fp16: 2250,
    bf16: 1,
    tf32: 1,
    int8: 4500,
    hbmCapacity: "192 GB",
    memoryBandwidth: "8 TB/s",
    tdp: "1000W",
    controlStatus: "Controlled",
    eccn: "3A090",
    notes: "Dual-die design (2×807mm²). TPP estimate from FP8 dense. Values preliminary.",
    sources: [
      { name: "NVIDIA GTC 2024", url: "#" },
      { name: "SemiAnalysis estimates", url: "#" }
    ]
  },
  {
    name: "NVIDIA H200 NVL",
    manufacturer: "NVIDIA",
    architecture: "Hopper",
    releaseDate: "2024",
    tpp: 13364,
    pd: 16.42,
    interconnect: "900 GB/s (NVLink)",
    fp4: null,
    fp8: "3341 (sparse)",
    fp16: "1671 (sparse)",
    bf16: "1671 (sparse)",
    tf32: "835 (sparse)",
    int8: "3341 (sparse)",
    dieArea: "814",
    hbmCapacity: "141 GB HBM3e",
    memoryBandwidth: "4.8 TB/s",
    tdp: "600W",
    controlStatus: "Exportable (special exception)",
    eccn: null,
    notes: "In December 2025, the Trump Administration announced that Nvidia will be allowed to sell H200 chips to China in exchange for a 25% surcharge.",
    sources: [
      { name: "H200 datasheet", url: "https://resources.nvidia.com/en-us-data-center-overview/hpc-datasheet-sc23-h200" },
      { name: "Semafor", url: "https://www.semafor.com/article/12/09/2025/trump-says-nvidia-can-sell-h200-ai-chips-to-china" }
    ]
  },
  {
    name: "NVIDIA H200 SXM",
    manufacturer: "NVIDIA",
    architecture: "Hopper",
    releaseDate: "2024",
    tpp: 15832,
    pd: 19.45,
    interconnect: "900 GB/s (NVLink)",
    fp4: null,
    fp8: "3958 (sparse)",
    fp16: "1979 (sparse)",
    bf16: "1979 (sparse)",
    tf32: "989 (sparse)",
    int8: "3958 (sparse)",
    dieArea: "814",
    hbmCapacity: "141 GB HBM3e",
    memoryBandwidth: "4.8 TB/s",
    tdp: "700W",
    controlStatus: "Exportable (special exception)",
    eccn: null,
    notes: "In December 2025, the Trump Administration announced that Nvidia will be allowed to sell H200 chips to China in exchange for a 25% surcharge.",
    sources: [
      { name: "H200 datasheet", url: "https://resources.nvidia.com/en-us-data-center-overview/hpc-datasheet-sc23-h200" },
      { name: "Semafor", url: "https://www.semafor.com/article/12/09/2025/trump-says-nvidia-can-sell-h200-ai-chips-to-china" }    
    ]
  },
  {
    name: "NVIDIA HGX H20",
    manufacturer: "NVIDIA",
    architecture: "Hopper",
    releaseDate: "2023",
    tpp: 2368,
    pd: 2.91,
    interconnect: "900 GB/s (NVLink)",
    fp4: null,
    fp8: "296",
    fp16: "148",
    bf16: "148",
    tf32: "74",
    int8: "296",
    dieArea: "814",
    hbmCapacity: "96 GB HBM3",
    memoryBandwidth: "4 TB/s",
    tdp: "400W",
    controlStatus: "Exportable (special exception)",
    eccn: null,
    notes: "China-specific Hopper chip with downgraded arithmetic performance to fall under TPP and PD thresholds in light of 2023 controls. However, the H20's inference-optimized features (enhanced interconnect and memory capacity/bandwidth) made it a powerful chip with the rise of reasoning models and test-time compute scaling. The US considered banning H20 sales to China, but the Trump Administration instead opted to allow exports in exchange for 15% of Nvidia's revenue from these sales.",
    sources: [
      { name: "Tom's Hardware", url: "https://www.tomshardware.com/news/nvidias-latest-regulation-compliant-gpu-for-china-has-been-delayed-to-early-next-year" },
      { name: "NPR", url: "https://www.npr.org/2025/08/11/nx-s1-5498689/trump-nvidia-h20-chip-sales-china" },    
    ]
  },
  {
    name: "NVIDIA H800 PCIe",
    manufacturer: "NVIDIA",
    architecture: "Hopper",
    releaseDate: "2023",
    tpp: 12104,
    pd: 14.87,
    interconnect: "600 GB/s (NVLink)",
    fp4: null,
    fp8: "3026 (sparse)",
    fp16: "1513 (sparse)",
    bf16: "1513 (sparse)",
    tf32: "756 (sparse)",
    int8: "3026 (sparse)",
    dieArea: "814",
    hbmCapacity: "80 GB HBM3",
    memoryBandwidth: "2 TB/s",
    tdp: "350W",
    controlStatus: "Controlled",
    eccn: "3A090.a",
    notes: "China-specific chip with same arithmetic performance as H100 but with reduced interconnect designed to circumvent 2022 export controls. Loophole closed in 2023 controls with removal of interconnect threshold and introduction of PD.",
    sources: [
      { name: "H800 datasheet", url: "https://www.chaoqing-i.com/upload/20231128/NVIDIA%20H800%20GPU%20Datasheet.pdf" }
    ]
  },
  {
    name: "NVIDIA H800 SXM",
    manufacturer: "NVIDIA",
    architecture: "Hopper",
    releaseDate: "2023",
    tpp: 15832,
    pd: 19.45,
    interconnect: "900 GB/s (NVLink)",
    fp4: null,
    fp8: "3958 (sparse)",
    fp16: "1979 (sparse)",
    bf16: "1979 (sparse)",
    tf32: "989 (sparse)",
    int8: "3958 (sparse)",
    dieArea: "814",
    hbmCapacity: "80 GB HBM3",
    memoryBandwidth: "3.35 TB/s",
    tdp: "700W",
    controlStatus: "Controlled",
    eccn: "3A090.a",
    notes: "China-specific chip with same arithmetic performance as H100 but with reduced interconnect designed to circumvent 2022 export controls. Loophole closed in 2023 controls with removal of interconnect threshold and introduction of PD.",
    sources: [
    { name: "H800 datasheet", url: "https://www.chaoqing-i.com/upload/20231128/NVIDIA%20H800%20GPU%20Datasheet.pdf" }
    ]
  },
  {
    name: "NVIDIA H100 NVL",
    manufacturer: "NVIDIA",
    architecture: "Hopper",
    releaseDate: "2022",
    tpp: 13364,
    pd: 16.42,
    interconnect: "600 GB/s (NVLink)",
    fp4: null,
    fp8: "3341 (sparse)",
    fp16: "1671 (sparse)",
    bf16: "1671 (sparse)",
    tf32: "835 (sparse)",
    int8: "3341 (sparse)",
    dieArea: "814",
    hbmCapacity: "80 GB HBM3",
    memoryBandwidth: "3.9 TB/s",
    tdp: "400W",
    controlStatus: "Controlled",
    eccn: "3A090.a",
    notes: ".",
    sources: [
      { name: "H100 datasheet", url: "https://resources.nvidia.com/en-us-gpu/h100-datasheet-24306" }
    ]
  },
  {
    name: "NVIDIA H100 SXM",
    manufacturer: "NVIDIA",
    architecture: "Hopper",
    releaseDate: "2022",
    tpp: 15832,
    pd: 19.45,
    interconnect: "900 GB/s (NVLink)",
    fp4: null,
    fp8: "3958 (sparse)",
    fp16: "1979 (sparse)",
    bf16: "1979 (sparse)",
    tf32: "989 (sparse)",
    int8: "3958 (sparse)",
    dieArea: "814",
    hbmCapacity: "80 GB HBM3",
    memoryBandwidth: "3.35 TB/s",
    tdp: "700W",
    controlStatus: "Controlled",
    eccn: "3A090.a",
    notes: ".",
    sources: [
    { name: "H100 datasheet", url: "https://resources.nvidia.com/en-us-gpu/h100-datasheet-24306" }
    ]
  },
  {
    name: "NVIDIA A100 SXM",
    manufacturer: "NVIDIA",
    architecture: "Ampere",
    releaseDate: "2020",
    tpp: 4992,
    interconnect: 826,
    pd: 6.0,
    fp4: 1,
    fp8: 4500,
    fp16: 2250,
    bf16: 1,
    tf32: 1,
    int8: 4500,
    hbmCapacity: "80 GB",
    memoryBandwidth: "2 TB/s",
    tdp: "400W",
    controlStatus: "Controlled",
    eccn: "3A090",
    notes: "No FP8 support. TPP from INT8 dense (624 TOPS × 8). Original Oct 2022 control target.",
    sources: [
      { name: "NVIDIA Ampere Architecture Whitepaper", url: "#" }
    ]
  },
  {
    name: "NVIDIA A800",
    manufacturer: "NVIDIA",
    architecture: "Ampere",
    releaseDate: "2022",
    tpp: 4992,
    interconnect: 826,
    pd: 6.0,
    fp4: 1,
    fp8: 4500,
    fp16: 2250,
    bf16: 1,
    tf32: 1,
    int8: 4500,
    hbmCapacity: "80 GB",
    memoryBandwidth: "1.55 TB/s",
    tdp: "400W",
    controlStatus: "Controlled (Oct 2023)",
    eccn: "3A090",
    notes: "China compliance variant with reduced interconnect. Controlled under Oct 2023 rule.",
    sources: [
      { name: "NVIDIA", url: "#" },
      { name: "BIS Final Rule Oct 2023", url: "#" }
    ]
  },
    {
    name: "AMD MI300X",
    manufacturer: "AMD",
    architecture: "CDNA 3",
    releaseDate: "2023",
    tpp: 10419,
    interconnect: 1536,
    pd: 6.8,
    fp4: 1,
    fp8: 4500,
    fp16: 2250,
    bf16: 1,
    tf32: 1,
    int8: 4500,
    hbmCapacity: "192 GB",
    memoryBandwidth: "5.3 TB/s",
    tdp: "750W",
    controlStatus: "Controlled",
    eccn: "3A090",
    notes: "Chiplet design: 8 XCDs (~1536mm² total logic). TPP from FP8 dense. Die area excludes I/O dies.",
    sources: [
      { name: "AMD CDNA 3 Whitepaper", url: "#" },
      { name: "AMD Tech Day 2023", url: "#" }
    ]
  },
  {
    name: "Huawei Ascend 910B",
    manufacturer: "Huawei",
    architecture: "Da Vinci",
    releaseDate: "2023",
    tpp: null,
    interconnect: null,
    pd: null,
    fp4: 1,
    fp8: 4500,
    fp16: 2250,
    bf16: 1,
    tf32: 1,
    int8: 4500,
    hbmCapacity: "64 GB",
    memoryBandwidth: "1.8 TB/s",
    tdp: "400W",
    controlStatus: "Entity List (Huawei)",
    eccn: "N/A",
    notes: "Specs unconfirmed. Huawei on Entity List since 2019. Manufactured by SMIC at 7nm.",
    sources: [
      { name: "Industry estimates", url: null },
      { name: "TechInsights teardown reports", url: "#" }
    ]
  }
];
