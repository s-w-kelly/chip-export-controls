/**
 * AI Chip Data
 *
 * To update this data:
 * - Add new chips by copying an existing object and modifying values
 * - All numeric fields (tpp, dieArea, pd, etc.) can be null if unknown
 * - controlStatus options: "Controlled", "Controlled (Oct 2023)", "Unknown", "Entity List (Company)"
 * - sources is an array of objects with { name, url } citing where data came from
 */

export const chipData = [
  {
    name: "NVIDIA B300",
    manufacturer: "NVIDIA",
    architecture: "Blackwell",
    releaseDate: "2025",
    tpp: 56000,
    pd: 34.40,
    interconnect: "1800 GB/s (NVLink)",
    fp4: 14000,
    fp8: 4500,
    fp16: 2250,
    bf16: 2250,
    tf32: 1100,
    int8: 154.5,
    dieArea: "1628 (estimate)",
    hbmCapacity: "270 GB HBM3e",
    memoryBandwidth: "7.7 TB/s",
    tdp: "1000W",
    controlStatus: "Controlled",
    eccn: "3A090.a",
    notes: "While the B300 has identical arithmetic performance for most bit lengths as the B200, Blackwell Ultra improved dense performance for FP4, so the B300 TPP is ~1.6x the B200.",
    sources: [
      { name: "Blackwell Ultra technical brief", url: "https://resources.nvidia.com/en-us-blackwell-architecture/blackwell-ultra-datasheet?ncid=no-ncid" },
      { name: "Tom's Hardware (die area estimate)", url: "https://www.tomshardware.com/pc-components/gpus/nvidias-next-gen-ai-gpu-revealed-blackwell-b200-gpu-delivers-up-to-20-petaflops-of-compute-and-massive-improvements-over-hopper-h100#:~:text=The%20reason%20for%20this%20dual,and%20Ada%20Lovelace%20architecture%20GPUs." },
    ]
  },
  {
    name: "NVIDIA B200",
    manufacturer: "NVIDIA",
    architecture: "Blackwell",
    releaseDate: "2024",
    tpp: 36000,
    pd: 22.11,
    interconnect: "1800 GB/s (NVLink)",
    fp4: 9000,
    fp8: 4500,
    fp16: 2250,
    bf16: 2250,
    tf32: 1100,
    int8: 2500,
    dieArea: "1628 (estimate)",
    hbmCapacity: "192 GB HBM3e",
    memoryBandwidth: "8 TB/s",
    tdp: "1000W",
    controlStatus: "Controlled",
    eccn: "3A090.a",
    notes: ".",
    sources: [
      { name: "Blackwell technical brief", url: "https://www.tech-odyssey.cn/pdf/nv-gpu/NVIDIA-Blackwell-Architecture-Technical-Overview.pdf#page=19" },
      { name: "Tom's Hardware (die area estimate)", url: "https://www.tomshardware.com/pc-components/gpus/nvidias-next-gen-ai-gpu-revealed-blackwell-b200-gpu-delivers-up-to-20-petaflops-of-compute-and-massive-improvements-over-hopper-h100#:~:text=The%20reason%20for%20this%20dual,and%20Ada%20Lovelace%20architecture%20GPUs." },
    ]
  },
  {
    name: "NVIDIA B100",
    manufacturer: "NVIDIA",
    architecture: "Blackwell",
    releaseDate: "2024",
    tpp: 28000,
    pd: 17.20,
    interconnect: "1800 GB/s (NVLink)",
    fp4: 7000,
    fp8: 3500,
    fp16: 1750,
    bf16: 1750,
    tf32: 900,
    int8: 3500,
    dieArea: "1628 (estimate)",
    hbmCapacity: "192 GB HBM3e",
    memoryBandwidth: "8 TB/s",
    tdp: "700W",
    controlStatus: "Controlled",
    eccn: "3A090.a",
    notes: ".",
    sources: [
      { name: "Blackwell technical brief", url: "https://www.tech-odyssey.cn/pdf/nv-gpu/NVIDIA-Blackwell-Architecture-Technical-Overview.pdf#page=19" },
      { name: "Tom's Hardware (die area estimate)", url: "https://www.tomshardware.com/pc-components/gpus/nvidias-next-gen-ai-gpu-revealed-blackwell-b200-gpu-delivers-up-to-20-petaflops-of-compute-and-massive-improvements-over-hopper-h100#:~:text=The%20reason%20for%20this%20dual,and%20Ada%20Lovelace%20architecture%20GPUs." },
    ]
  },
  {
    name: "NVIDIA H200 NVL",
    manufacturer: "NVIDIA",
    architecture: "Hopper",
    releaseDate: "2024",
    tpp: 13364,
    pd: 16.42,
    interconnect: "900 GB/s (NVLink)",
    fp4: null,
    fp8: "3341 (sparse)",
    fp16: "1671 (sparse)",
    bf16: "1671 (sparse)",
    tf32: "835 (sparse)",
    int8: "3341 (sparse)",
    dieArea: "814",
    hbmCapacity: "141 GB HBM3e",
    memoryBandwidth: "4.8 TB/s",
    tdp: "600W",
    controlStatus: "Exportable\n(special exception)",
    eccn: null,
    notes: "In December 2025, the Trump Administration announced that Nvidia will be allowed to sell H200 chips to China in exchange for a 25% surcharge.",
    sources: [
      { name: "H200 datasheet", url: "https://resources.nvidia.com/en-us-data-center-overview/hpc-datasheet-sc23-h200" },
      { name: "NVIDIA Technical Blog (die area)", url: "https://developer.nvidia.com/blog/nvidia-hopper-architecture-in-depth/" },    
      { name: "Semafor (Trump deal)", url: "https://www.semafor.com/article/12/09/2025/trump-says-nvidia-can-sell-h200-ai-chips-to-china" }
    ]
  },
  {
    name: "NVIDIA H200 SXM",
    manufacturer: "NVIDIA",
    architecture: "Hopper",
    releaseDate: "2024",
    tpp: 15832,
    pd: 19.45,
    interconnect: "900 GB/s (NVLink)",
    fp4: null,
    fp8: "3958 (sparse)",
    fp16: "1979 (sparse)",
    bf16: "1979 (sparse)",
    tf32: "989 (sparse)",
    int8: "3958 (sparse)",
    dieArea: "814",
    hbmCapacity: "141 GB HBM3e",
    memoryBandwidth: "4.8 TB/s",
    tdp: "700W",
    controlStatus: "Exportable\n(special exception)",
    eccn: null,
    notes: "In December 2025, the Trump Administration announced that Nvidia will be allowed to sell H200 chips to China in exchange for a 25% surcharge.",
    sources: [
      { name: "H200 datasheet", url: "https://resources.nvidia.com/en-us-data-center-overview/hpc-datasheet-sc23-h200" },
      { name: "NVIDIA Technical Blog (die area)", url: "https://developer.nvidia.com/blog/nvidia-hopper-architecture-in-depth/" },    
      { name: "Semafor (Trump deal)", url: "https://www.semafor.com/article/12/09/2025/trump-says-nvidia-can-sell-h200-ai-chips-to-china" }
    ]
  },
  {
    name: "NVIDIA H20 SXM",
    manufacturer: "NVIDIA",
    architecture: "Hopper",
    releaseDate: "2023",
    tpp: 2368,
    pd: 2.91,
    interconnect: "900 GB/s (NVLink)",
    fp4: null,
    fp8: "296",
    fp16: "148",
    bf16: "148",
    tf32: "74",
    int8: "296",
    dieArea: "814",
    hbmCapacity: "96 GB HBM3",
    memoryBandwidth: "4 TB/s",
    tdp: "400W",
    controlStatus: "Exportable\n(special exception)",
    eccn: null,
    notes: "China-specific Hopper chip with downgraded arithmetic performance to fall under TPP and PD thresholds in light of 2023 controls. However, the H20's inference-optimized features (enhanced interconnect and memory capacity/bandwidth) made it a powerful chip with the rise of reasoning models and test-time compute scaling. The US considered banning H20 sales to China, but the Trump Administration instead opted to allow exports in exchange for 15% of Nvidia's revenue from these sales.",
    sources: [
      { name: "Tom's Hardware", url: "https://www.tomshardware.com/news/nvidias-latest-regulation-compliant-gpu-for-china-has-been-delayed-to-early-next-year" },
      { name: "NVIDIA Technical Blog (die area)", url: "https://developer.nvidia.com/blog/nvidia-hopper-architecture-in-depth/" },    
      { name: "NPR (Trump deal)", url: "https://www.npr.org/2025/08/11/nx-s1-5498689/trump-nvidia-h20-chip-sales-china" },    
    ]
  },
  {
    name: "NVIDIA H800 PCIe",
    manufacturer: "NVIDIA",
    architecture: "Hopper",
    releaseDate: "2023",
    tpp: 12104,
    pd: 14.87,
    interconnect: "600 GB/s (NVLink)",
    fp4: null,
    fp8: "3026 (sparse)",
    fp16: "1513 (sparse)",
    bf16: "1513 (sparse)",
    tf32: "756 (sparse)",
    int8: "3026 (sparse)",
    dieArea: "814",
    hbmCapacity: "80 GB HBM3",
    memoryBandwidth: "2 TB/s",
    tdp: "350W",
    controlStatus: "Controlled",
    eccn: "3A090.a",
    notes: "China-specific chip with same arithmetic performance as H100 but with reduced interconnect designed to circumvent 2022 export controls. Loophole closed in 2023 controls with removal of interconnect threshold and introduction of PD.",
    sources: [
      { name: "H800 datasheet", url: "https://www.chaoqing-i.com/upload/20231128/NVIDIA%20H800%20GPU%20Datasheet.pdf" },
      { name: "NVIDIA Technical Blog (die area)", url: "https://developer.nvidia.com/blog/nvidia-hopper-architecture-in-depth/" },
    ]
  },
  {
    name: "NVIDIA H800 SXM",
    manufacturer: "NVIDIA",
    architecture: "Hopper",
    releaseDate: "2023",
    tpp: 15832,
    pd: 19.45,
    interconnect: "900 GB/s (NVLink)",
    fp4: null,
    fp8: "3958 (sparse)",
    fp16: "1979 (sparse)",
    bf16: "1979 (sparse)",
    tf32: "989 (sparse)",
    int8: "3958 (sparse)",
    dieArea: "814",
    hbmCapacity: "80 GB HBM3",
    memoryBandwidth: "3.35 TB/s",
    tdp: "700W",
    controlStatus: "Controlled",
    eccn: "3A090.a",
    notes: "China-specific chip with same arithmetic performance as H100 but with reduced interconnect designed to circumvent 2022 export controls. Loophole closed in 2023 controls with removal of interconnect threshold and introduction of PD.",
    sources: [
    { name: "H800 datasheet", url: "https://www.chaoqing-i.com/upload/20231128/NVIDIA%20H800%20GPU%20Datasheet.pdf" },
    { name: "NVIDIA Technical Blog (die area)", url: "https://developer.nvidia.com/blog/nvidia-hopper-architecture-in-depth/" },
    ]
  },
  {
    name: "NVIDIA H100 NVL",
    manufacturer: "NVIDIA",
    architecture: "Hopper",
    releaseDate: "2022",
    tpp: 13364,
    pd: 16.42,
    interconnect: "600 GB/s (NVLink)",
    fp4: null,
    fp8: "3341 (sparse)",
    fp16: "1671 (sparse)",
    bf16: "1671 (sparse)",
    tf32: "835 (sparse)",
    int8: "3341 (sparse)",
    dieArea: "814",
    hbmCapacity: "80 GB HBM3",
    memoryBandwidth: "3.9 TB/s",
    tdp: "400W",
    controlStatus: "Controlled",
    eccn: "3A090.a",
    notes: ".",
    sources: [
      { name: "H100 datasheet", url: "https://resources.nvidia.com/en-us-gpu/h100-datasheet-24306" },
      { name: "NVIDIA Technical Blog (die area)", url: "https://developer.nvidia.com/blog/nvidia-hopper-architecture-in-depth/" },
        ]
  },
  {
    name: "NVIDIA H100 SXM",
    manufacturer: "NVIDIA",
    architecture: "Hopper",
    releaseDate: "2022",
    tpp: 15832,
    pd: 19.45,
    interconnect: "900 GB/s (NVLink)",
    fp4: null,
    fp8: "3958 (sparse)",
    fp16: "1979 (sparse)",
    bf16: "1979 (sparse)",
    tf32: "989 (sparse)",
    int8: "3958 (sparse)",
    dieArea: "814",
    hbmCapacity: "80 GB HBM3",
    memoryBandwidth: "3.35 TB/s",
    tdp: "700W",
    controlStatus: "Controlled",
    eccn: "3A090.a",
    notes: ".",
    sources: [
    { name: "H100 datasheet", url: "https://resources.nvidia.com/en-us-gpu/h100-datasheet-24306" },
    { name: "NVIDIA Technical Blog (die area)", url: "https://developer.nvidia.com/blog/nvidia-hopper-architecture-in-depth/" },
    ]
  },
  {
    name: "NVIDIA A800 PCIe",
    manufacturer: "NVIDIA",
    architecture: "Ampere",
    releaseDate: "2022",
    tpp: 4992,
    pd: 6.04,
    interconnect: "400 GB/s (NVLink)",
    fp4: null,
    fp8: null,
    fp16: "312",
    bf16: "312",
    tf32: "156",
    int8: "624",
    dieArea: "826",
    hbmCapacity: "80 GB HBM2e",
    memoryBandwidth: "1.9 TB/s",
    tdp: "250W",
    controlStatus: "Controlled",
    eccn: "3A090.a",
    notes: "China-specific chip with same arithmetic performance as A100 but with reduced interconnect designed to circumvent 2022 export controls. Loophole closed in 2023 controls with removal of interconnect threshold and introduction of PD.",
    sources: [
      { name: "A800 datasheet", url: "https://img.deepbaytech.com/NVIDIA%20A800%20TENSOR%20CORE%20GPU.pdf" }
    ]
  },
  {
    name: "NVIDIA A800 SXM",
    manufacturer: "NVIDIA",
    architecture: "Ampere",
    releaseDate: "2022",
    tpp: 4992,
    pd: 6.04,
    interconnect: "400 GB/s (NVLink)",
    fp4: null,
    fp8: null,
    fp16: "312",
    bf16: "312",
    tf32: "156",
    int8: "624",
    dieArea: "826",
    hbmCapacity: "80 GB HBM2e",
    memoryBandwidth: "2 TB/s",
    tdp: "400W",
    controlStatus: "Controlled",
    eccn: "3A090.a",
    notes: "China-specific chip with same arithmetic performance as H100 but with reduced interconnect designed to circumvent 2022 export controls. Loophole closed in 2023 controls with removal of interconnect threshold and introduction of PD.",
    sources: [
    { name: "A800 datasheet", url: "https://img.deepbaytech.com/NVIDIA%20A800%20TENSOR%20CORE%20GPU.pdf" }
    ]
  },
  {
    name: "NVIDIA A100 PCIe",
    manufacturer: "NVIDIA",
    architecture: "Ampere",
    releaseDate: "2020",
    tpp: 4992,
    pd: 6.04,
    interconnect: "600 GB/s (NVLink)",
    fp4: null,
    fp8: null,
    fp16: "312",
    bf16: "312",
    tf32: "156",
    int8: "624",
    dieArea: "826",
    hbmCapacity: "40 GB HBM2",
    memoryBandwidth: "1.6 TB/s",
    tdp: "250W",
    controlStatus: "Controlled",
    eccn: "3A090.a",
    notes: "Primary target of 2022 controls, with intial thresholds essentially set right at A100 levels (TPP limit of 4800 right below A100 TPP, interconnect limit of 600 GB/s right at A100 mark).",
    sources: [
      { name: "A100 datasheet", url: "https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/a100/pdf/nvidia-a100-datasheet.pdf." }
    ]
  },
  {
    name: "NVIDIA A100 SXM",
    manufacturer: "NVIDIA",
    architecture: "Ampere",
    releaseDate: "2020",
    tpp: 4992,
    pd: 6.04,
    interconnect: "600 GB/s (NVLink)",
    fp4: null,
    fp8: null,
    fp16: "312",
    bf16: "312",
    tf32: "156",
    int8: "624",
    dieArea: "826",
    hbmCapacity: "40 GB HBM2",
    memoryBandwidth: "1.6 TB/s",
    tdp: "400W",
    controlStatus: "Controlled",
    eccn: "3A090.a",
    notes: "Primary target of 2022 controls, with intial thresholds essentially set right at A100 levels (TPP limit of 4800 right below A100 TPP, interconnect limit of 600 GB/s right at A100 mark).",
    sources: [
    { name: "A100 datasheet", url: "https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/a100/pdf/nvidia-a100-datasheet.pdf." }
    ]
  },
  {
    name: "AMD MI355X",
    manufacturer: "AMD",
    architecture: "CDNA 4",
    releaseDate: "2025",
    tpp: 40265,
    pd: 39.59,
    interconnect: "1075.2 GB/s (Infinity Fabric)",
    fp4: 10066.3,
    fp8: 5033.2,
    fp16: 2516.6,
    bf16: 2516.6,
    tf32: null,
    int8: 5033.2,
    dieArea: "1017 (estimate)",
    hbmCapacity: "288 GB HBM3e",
    memoryBandwidth: "8 TB/s",
    tdp: "1400W",
    controlStatus: "Controlled",
    eccn: "3A090.a",
    notes: "",
    sources: [
    { name: "MI355X datasheet", url: "https://www.amd.com/content/dam/amd/en/documents/instinct-tech-docs/product-briefs/amd-instinct-mi355x-gpu-brochure.pdf" },    
  ]
  },
  {
    name: "AMD MI350X",
    manufacturer: "AMD",
    architecture: "CDNA 4",
    releaseDate: "2025",
    tpp: 36910,
    pd: 36.29,
    interconnect: "896 GB/s (Infinity Fabric)",
    fp4: 9227.5,
    fp8: 4614,
    fp16: 2309.6,
    bf16: 2309.6,
    tf32: null,
    int8: 4614,
    dieArea: "1017 (estimate)",
    hbmCapacity: "288 GB HBM3e",
    memoryBandwidth: "8 TB/s",
    tdp: "1000W",
    controlStatus: "Controlled",
    eccn: "3A090.a",
    notes: "",
    sources: [
    { name: "MI300X datasheet", url: "https://www.amd.com/content/dam/amd/en/documents/instinct-tech-docs/product-briefs/amd-instinct-mi350x-gpu-brochure.pdf" },  ]
  },
  {
    name: "AMD MI325X",
    manufacturer: "AMD",
    architecture: "CDNA 3",
    releaseDate: "2024",
    tpp: 20919,
    pd: 20.57,
    interconnect: "896 GB/s (Infinity Fabric)",
    fp4: null,
    fp8: 2614.9,
    fp16: 1307.4,
    bf16: 1307.4,
    tf32: 653.7,
    int8: 2614.9,
    dieArea: 1017,
    hbmCapacity: "256 GB HBM3e",
    memoryBandwidth: "6 TB/s",
    tdp: "1000W",
    controlStatus: "Controlled",
    eccn: "3A090.a",
    notes: "",
    sources: [
    { name: "MI325X datasheet", url: "https://www.amd.com/content/dam/amd/en/documents/instinct-tech-docs/product-briefs/instinct-mi325x-datasheet.pdf" },
    { name: "TechPowerUp (die area)", url: "https://www.techpowerup.com/gpu-specs/radeon-instinct-mi300x.c4179" },    
  ]
  },
  {
    name: "AMD MI300X",
    manufacturer: "AMD",
    architecture: "CDNA 3",
    releaseDate: "2023",
    tpp: 20919,
    pd: 20.57,
    interconnect: "896 GB/s (Infinity Fabric)",
    fp4: null,
    fp8: 2614.9,
    fp16: 312,
    bf16: 312,
    tf32: 653.7,
    int8: 2614.9,
    dieArea: 1017,
    hbmCapacity: "192 GB HBM3",
    memoryBandwidth: "5.3 TB/s",
    tdp: "750W",
    controlStatus: "Controlled",
    eccn: "3A090.a",
    notes: "",
    sources: [
    { name: "MI300X datasheet", url: "https://www.amd.com/content/dam/amd/en/documents/instinct-tech-docs/data-sheets/amd-instinct-mi300x-data-sheet.pdf" },
    { name: "TechPowerUp (die area)", url: "https://www.techpowerup.com/gpu-specs/radeon-instinct-mi300x.c4179" },
  ]
  },
  {
    name: "AMD MI300A",
    manufacturer: "AMD",
    architecture: "CDNA 3",
    releaseDate: "2023",
    tpp: 15690,
    pd: 15.43,
    interconnect: "512 GB/s (Infinity Fabric)",
    fp4: null,
    fp8: 1961.2,
    fp16: 980.6,
    bf16: 980.6,
    tf32: 490.3,
    int8: 1961.2,
    dieArea: 1017,
    hbmCapacity: "128 GB HBM3",
    memoryBandwidth: "5.3 TB/s",
    tdp: "760W",
    controlStatus: "Controlled",
    eccn: "3A090.a",
    notes: "",
    sources: [
    { name: "MI300A datasheet", url: "https://www.amd.com/content/dam/amd/en/documents/instinct-tech-docs/data-sheets/amd-instinct-mi300a-data-sheet.pdf" },
    { name: "TechPowerUp (die area)", url: "https://www.techpowerup.com/gpu-specs/radeon-instinct-mi300x.c4179" },
    ]
  },
];
